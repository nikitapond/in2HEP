{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. $H\\rightarrow b\\bar{b}$ via Machine Learning \n",
    "\n",
    "A couple of days ago, we used a neural network to distinguish between handwritten characters. Today, we shall instead use a neural network to distinguish between events that contain a Higgs boson decay (signal) or some background process. \n",
    "\n",
    "## 1.1 Neural Networks in Keras\n",
    "\n",
    "The algorithm for doing the $H\\rightarrow b\\bar{b}$ can be summarised with the following steps:\n",
    "\n",
    "* Import libraries and load data.\n",
    "* Build neural network. \n",
    "* Training Neural Network. \n",
    "* Make decisions about events using the Neural Network. \n",
    "* Calculate sensitivity. \n",
    "\n",
    "\n",
    "## 1.2 Importing libraries and loading \n",
    "\n",
    "The Keras library is a free open-source tool to quickly and easily develop with neural networks. It does a lot of the heavy lifting for us making it an ideal tool for prototyping. Just like any popular Python library, it is well documented with guides explaining how to use it. These are available here: https://keras.io/\n",
    "\n",
    "In this exercise, your mentors will guide you through a lot of the code that has already been written for you so that you can focus specifically on building the neural network. It is still valuable to understand what is going on so make sure you ask questions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-27 09:12:29--  https://raw.githubusercontent.com/nikitapond/in2HEP/master/data-v1/VHbb_data_2jet.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44311357 (42M) [text/plain]\n",
      "Saving to: ‘VHbb_data_2jet.csv.9’\n",
      "\n",
      "VHbb_data_2jet.csv. 100%[===================>]  42.26M  8.60MB/s    in 5.0s    \n",
      "\n",
      "2022-07-27 09:12:34 (8.40 MB/s) - ‘VHbb_data_2jet.csv.9’ saved [44311357/44311357]\n",
      "\n",
      "--2022-07-27 09:12:34--  https://raw.githubusercontent.com/nikitapond/in2HEP/master/notebooks/ucl_masterclass.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25153 (25K) [text/plain]\n",
      "Saving to: ‘ucl_masterclass.py.10’\n",
      "\n",
      "ucl_masterclass.py. 100%[===================>]  24.56K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2022-07-27 09:12:34 (5.44 MB/s) - ‘ucl_masterclass.py.10’ saved [25153/25153]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/nikitapond/in2HEP/master/data-v1/VHbb_data_2jet.csv\n",
    "!wget https://raw.githubusercontent.com/nikitapond/in2HEP/master/notebooks/ucl_masterclass.py\n",
    "    \n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from ucl_masterclass import *\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_yaml\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras import backend as K\n",
    "from time import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Training and testing dataset\n",
    "\n",
    "The neural network is given a set of training events that have a Class label of 1 for signal and 0 for background. During the training phase, the neural network goal is to *learn* how to map input variables given to it (kinematic and topological quantities of the event) to 0 for background and 1 for signal. \n",
    "\n",
    "To ensure that the neural network is learning general features about the signal and background process and not just **artifacts** unique to the training data set, we split the data set into a training and testing set. We train our model on the training dataset, and evaluate its performance on the testing dataset. We split the dataset in the next couple of code cells.\n",
    "\n",
    "**In the section below we extract the input (x) and target values (y) used to train the neural network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables we wish to use\n",
    "variables = ['dRBB','mBB','MET','Mtop','pTV',]\n",
    "num_variables = len(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data-v1/VHbb_data_2jet.csv')\n",
    "\n",
    "# Define what fraction of the dataset is for testing\n",
    "test_frac = 0.5\n",
    "\n",
    "# Split the dataset into a training/testing split\n",
    "df_train, df_test = train_test_split(df, test_size=test_frac)\n",
    "df_test = deepcopy(df_test)\n",
    "\n",
    "# Get the X, Y, and weights for training dataset\n",
    "X_train = scale(df_train[variables].values)\n",
    "Y_train = df_train['Class'].values\n",
    "W_train = df_train['training_weight'].values\n",
    "\n",
    "# We only need the X_test from the test dataset\n",
    "X_test = scale(df_test[variables].values)\n",
    "# Scale event weights by dataset fraction\n",
    "df_test['EventWeight'] /= test_frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training the model\n",
    "Now our data is ready, we can define our model, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1842/1842 [==============================] - 2s 1ms/step - loss: 0.5333 - accuracy: 0.7320\n",
      "Epoch 2/5\n",
      "1842/1842 [==============================] - 2s 1ms/step - loss: 0.4415 - accuracy: 0.7927\n",
      "Epoch 3/5\n",
      "1842/1842 [==============================] - 2s 1ms/step - loss: 0.3840 - accuracy: 0.8493\n",
      "Epoch 4/5\n",
      "1842/1842 [==============================] - 2s 1ms/step - loss: 0.3671 - accuracy: 0.8578\n",
      "Epoch 5/5\n",
      "1842/1842 [==============================] - 2s 1ms/step - loss: 0.3589 - accuracy: 0.8607\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # An input layer that has the same size, as number of variables used\n",
    "  Dense(units=num_variables,input_dim = num_variables,activation='relu'), \n",
    "  Dense(16, activation='relu'), # A single hidden layer with 16 neurons\n",
    "  Dense(1, activation='sigmoid') # An output layer of size 1\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, sample_weight=W_train, epochs=5, batch_size=32)\n",
    "\n",
    "df_test['decision_value'] = model.predict(X_test)\n",
    "print(f\"NN sensitivity: {round(sensitivity_NN(df_test)[0],2)}\")\n",
    "nn_output_plot(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already get a good sensitivity compared to the cut based approach attempted yesterday.\n",
    "\n",
    "### Task 1 - Sensitivity variance\n",
    "Try running the two code cells above a few times (creating the model, and evaluting its sensitivity, add each sensitivity to the list below), what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your sensitivities here\n",
    "sensitivities = []\n",
    "\n",
    "# Turn list into numpy array for better scientific use\n",
    "sensitivities = np.array(sensitivities)\n",
    "\n",
    "# Find mean and standard deviation of sensitivity\n",
    "mean = round(np.mean(sensitivities),3)\n",
    "std = round(np.std(sensitivities),3)\n",
    "\n",
    "print(f\"The average sensitivity was {mean} +- {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in all experiments, the outcome will change a little bit every time. In a physical experiment, this might be due to slight measurement errors, or fluctuations in the environment. In a machine learning model, it is due to the way the model learns, which has some random elements to it. This results in slightly different outputs for the model each time we train it.\n",
    "To get a better idea of if 1 model is actually better than another, we can train a model multiple times and take an average of the result.\n",
    "\n",
    "We could do this in a new cell, and write a for loop and take an average. Instead, we introduce a function below that uses a function to build the model, and then automatically trains it several times and returns the average sensitivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateAverageSensitivity(classifier, epochs, batchsize, iterations=3):\n",
    "    '''\n",
    "    Creates and trains a model multiple times, calculates the average sensitivity, and \n",
    "    the standard deviation in the sensitivity\n",
    "    \n",
    "    Parameters:\n",
    "        classifier - a function the returns a tensorflow model\n",
    "        epochs - the number of epochs to train for\n",
    "        batchsize - the batchsize per epoch\n",
    "        iterations - the number of models to train and take an average of\n",
    "    Returns - mean, std\n",
    "        mean - the mean average sensitivity\n",
    "        std - the standard deviation in the sensitivity\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    for i in range(iterations):\n",
    "        model = classifier()\n",
    "        model.fit(X_train, Y_train, sample_weight=W_train, epochs=5, batch_size=32, verbose=0)\n",
    "        df_test['decision_value'] = model.predict(X_test)\n",
    "        sense  = round(sensitivity_NN(df_test)[0],3)\n",
    "        print(f\"Model {i} finished training with sensitivity {sense}\")\n",
    "        results.append(sense)\n",
    "    \n",
    "    results = np.array(results)\n",
    "    return np.mean(results), np.std(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architecture \n",
    "def classifier():\n",
    "    \"\"\"\n",
    "    Creates a model for higgs to bb classification\n",
    "    \n",
    "    returns: Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "    # An input layer that has the same size, as number of variables used\n",
    "      Dense(units=num_variables,input_dim = num_variables,activation='relu'), \n",
    "      Dense(16, activation='relu'), # A single hidden layer with 16 neurons\n",
    "      Dense(1, activation='sigmoid') \n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = CalculateAverageSensitivity(classifier, 5, 32)\n",
    "print(f\"\\nThe model sensitivity was {round(mean,2)} +- {round(std,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Improve the model\n",
    "Now that we can easily find the average sensitivity, we can try and improve our model. Below, we write another classifier function that creates a new model. Try changing some of the following parameters to try and improve performance:\n",
    "* Number of hidden layers\n",
    "* Number of neurons in hidden layers\n",
    "* Activation function in hidden layers\n",
    "* Optimizer function\n",
    "* Number of epochs\n",
    "* Batch size\n",
    "\n",
    "Write down the variables you changed, and the sensitivity, in the following code cell, so you can plot your results.\n",
    "\n",
    "If you're confident with this already, you can try a for loop based approach for testing different parameters, as was shown in the final task of the MNIST neural network a couple of days ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architecture \n",
    "def classifier():\n",
    "    \"\"\"\n",
    "    Creates a model for higgs to bb classification\n",
    "    \n",
    "    returns: Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "    # An input layer that has the same size, as number of variables used\n",
    "      Dense(units=num_variables,input_dim = num_variables,activation='relu'), \n",
    "      Dense(16, activation='relu'), # A single hidden layer with 16 neurons\n",
    "      Dense(1, activation='sigmoid') # An output layer of size 10, for the 10 possible digits\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "epochs = 5\n",
    "batchsize = 32\n",
    "\n",
    "mean, std = CalculateAverageSensitivity(classifier, epochs, batchsize)\n",
    "print(f\"\\nThe model sensitivity was {round(mean,2)} +- {round(std,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_changed = [0, 1, 2, 3]\n",
    "sensitivity = [7,3,4,1]\n",
    "\n",
    "plt.plot(var_changed, sensitivity)\n",
    "# Add a title, and axis labels\n",
    "\n",
    "\n",
    "\n",
    "# save the figure to pdf - change the name so you don't overwrite your plots!\n",
    "plt.savefig(\"atlas_sense_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Neural Networks can learn correlations between variables and create a better understanding of the classification process hence leading to better signal sensitivity. They have been very popular in industry and regularly win competitions on the Kaggle machine learning forum. If you are interested in doing more machine learning this is a free 'nanodegree' on deep learning: https://www.youtube.com/watch?v=vOppzHpvTiQ&list=PL2-dafEMk2A7YdKv4XfKpfbTH5z6rEEj3\n",
    "\n",
    "**This material was produced by hackingEducation**  \n",
    "<img src=\"images/logo-black.png\" width=\"50\" align = 'left'/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
